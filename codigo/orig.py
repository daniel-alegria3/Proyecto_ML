import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de visualizaci√≥n
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class TorreGradienteML:
    def __init__(self, csv_path):
        """
        Inicializa la clase para an√°lisis de ML de datos de Torre de Gradiente

        Definici√≥n del problema:
        - Tipo: REGRESI√ìN
        - Objetivo: Predecir el flujo de calor del suelo (soil_heat) basado en variables meteorol√≥gicas
        - Variables predictoras: temperatura, humedad, viento en diferentes niveles
        """
        self.csv_path = csv_path
        self.df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.models = {}
        self.results = {}

    def load_and_explore_data(self):
        """Carga y realiza an√°lisis exploratorio de datos"""
        print("=" * 60)
        print("AN√ÅLISIS EXPLORATORIO DE DATOS")
        print("=" * 60)

        # Cargar datos
        try:
            self.df = pd.read_csv(self.csv_path)
            print(f"‚úì Datos cargados exitosamente: {self.df.shape}")
        except Exception as e:
            print(f"‚ùå Error al cargar datos: {e}")
            return

        # Informaci√≥n b√°sica del dataset
        print(f"\nüìä INFORMACI√ìN GENERAL:")
        print(f"   ‚Ä¢ Dimensiones: {self.df.shape[0]} filas x {self.df.shape[1]} columnas")
        print(f"   ‚Ä¢ Per√≠odo: {self.df['year'].min()}-{self.df['year'].max()}")
        print(f"   ‚Ä¢ Memoria utilizada: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

        # Verificar columnas esperadas seg√∫n metadatos
        expected_cols = ['FECHA_CORTE', 'UBIGEO', 'year', 'month', 'day',
                        'hour', 'temp_n1', 'temp_n2', 'temp_n3', 'temp_n4', 'temp_n5',
                        'temp_n6', 'wind_n1', 'wind_n2', 'wind_n3', 'wind_n4', 'wind_n5',
                        'wind_n6', 'RH_n1', 'RH_n2', 'RH_n3', 'RH_n4', 'RH_n5', 'RH_n6',
                        'dir_wind_01', 'dir_wind_02', 'soil_heat']

        print(f"\nüìã COLUMNAS DISPONIBLES:")
        available_cols = list(self.df.columns)
        for i, col in enumerate(available_cols):
            print(f"   {i+1:2d}. {col}")

        # Estad√≠sticas descriptivas
        print(f"\nüìà ESTAD√çSTICAS DESCRIPTIVAS:")
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        print(self.df[numeric_cols].describe())

        # Valores faltantes
        print(f"\nüîç VALORES FALTANTES:")
        missing_data = self.df.isnull().sum()
        missing_percent = (missing_data / len(self.df)) * 100
        missing_df = pd.DataFrame({
            'Columna': missing_data.index,
            'Valores Faltantes': missing_data.values,
            'Porcentaje': missing_percent.values
        })
        missing_df = missing_df[missing_df['Valores Faltantes'] > 0].sort_values('Porcentaje', ascending=False)
        if not missing_df.empty:
            print(missing_df.to_string(index=False))
        else:
            print("   ‚úì No hay valores faltantes")

        return self.df

    def create_visualizations(self):
        """Crea visualizaciones para an√°lisis exploratorio"""
        if self.df is None:
            print("‚ùå Primero debe cargar los datos")
            return

        print("\n" + "=" * 60)
        print("VISUALIZACIONES DE AN√ÅLISIS EXPLORATORIO")
        print("=" * 60)

        # Configurar subplots
        fig = plt.figure(figsize=(20, 15))

        # 1. Distribuci√≥n de la variable objetivo (soil_heat)
        if 'soil_heat' in self.df.columns:
            plt.subplot(3, 3, 1)
            plt.hist(self.df['soil_heat'].dropna(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            plt.title('Distribuci√≥n del Flujo de Calor del Suelo', fontsize=12, fontweight='bold')
            plt.xlabel('Flujo de Calor (W/m¬≤)')
            plt.ylabel('Frecuencia')
            plt.grid(True, alpha=0.3)

        # 2. Series temporales de temperatura
        temp_cols = [col for col in self.df.columns if 'temp_n' in col]
        if temp_cols:
            plt.subplot(3, 3, 2)
            for col in temp_cols[:3]:  # Solo primeros 3 niveles
                if col in self.df.columns:
                    plt.plot(self.df.index[:1000], self.df[col][:1000], alpha=0.7, label=col)
            plt.title('Series Temporales de Temperatura (Primeros 1000 puntos)', fontsize=12, fontweight='bold')
            plt.xlabel('√çndice Temporal')
            plt.ylabel('Temperatura (¬∞C)')
            plt.legend()
            plt.grid(True, alpha=0.3)

        # 3. Boxplot de velocidades de viento
        wind_cols = [col for col in self.df.columns if 'wind_n' in col]
        if wind_cols:
            plt.subplot(3, 3, 3)
            wind_data = [self.df[col].dropna().values for col in wind_cols if col in self.df.columns]
            plt.boxplot(wind_data, labels=wind_cols)
            plt.title('Distribuci√≥n de Velocidades de Viento por Nivel', fontsize=12, fontweight='bold')
            plt.xlabel('Nivel de Medici√≥n')
            plt.ylabel('Velocidad del Viento (m/s)')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)

        # 4. Humedad relativa por nivel
        rh_cols = [col for col in self.df.columns if 'RH_n' in col]
        if rh_cols:
            plt.subplot(3, 3, 4)
            rh_data = [self.df[col].dropna().values for col in rh_cols if col in self.df.columns]
            plt.boxplot(rh_data, labels=rh_cols)
            plt.title('Distribuci√≥n de Humedad Relativa por Nivel', fontsize=12, fontweight='bold')
            plt.xlabel('Nivel de Medici√≥n')
            plt.ylabel('Humedad Relativa (%)')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)

        # 5. Correlaci√≥n entre variables
        plt.subplot(3, 3, 5)
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        correlation_matrix = self.df[numeric_cols].corr()
        sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,
                   square=True, fmt='.2f', cbar_kws={'shrink': .8})
        plt.title('Matriz de Correlaci√≥n', fontsize=12, fontweight='bold')

        # 6. Variaci√≥n diurna promedio
        if 'hour' in self.df.columns and 'soil_heat' in self.df.columns:
            plt.subplot(3, 3, 6)
            hourly_avg = self.df.groupby('hour')['soil_heat'].mean()
            plt.plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, markersize=4)
            plt.title('Variaci√≥n Diurna Promedio del Flujo de Calor', fontsize=12, fontweight='bold')
            plt.xlabel('Hora del D√≠a')
            plt.ylabel('Flujo de Calor Promedio (W/m¬≤)')
            plt.grid(True, alpha=0.3)
            plt.xticks(range(0, 24, 2))

        # 7. Scatter plot: Temperatura vs Flujo de Calor
        if 'temp_n1' in self.df.columns and 'soil_heat' in self.df.columns:
            plt.subplot(3, 3, 7)
            plt.scatter(self.df['temp_n1'], self.df['soil_heat'], alpha=0.5, s=1)
            plt.title('Temperatura vs Flujo de Calor del Suelo', fontsize=12, fontweight='bold')
            plt.xlabel('Temperatura Nivel 1 (¬∞C)')
            plt.ylabel('Flujo de Calor (W/m¬≤)')
            plt.grid(True, alpha=0.3)

        # 8. Variaci√≥n estacional
        if 'month' in self.df.columns and 'soil_heat' in self.df.columns:
            plt.subplot(3, 3, 8)
            monthly_avg = self.df.groupby('month')['soil_heat'].mean()
            plt.bar(monthly_avg.index, monthly_avg.values, color='lightcoral', alpha=0.7)
            plt.title('Variaci√≥n Estacional del Flujo de Calor', fontsize=12, fontweight='bold')
            plt.xlabel('Mes')
            plt.ylabel('Flujo de Calor Promedio (W/m¬≤)')
            plt.grid(True, alpha=0.3)
            plt.xticks(range(1, 13))

        # 9. Distribuci√≥n de direcci√≥n del viento
        if 'dir_wind_01' in self.df.columns:
            plt.subplot(3, 3, 9)
            wind_dir = self.df['dir_wind_01'].dropna()
            plt.hist(wind_dir, bins=36, alpha=0.7, color='lightgreen', edgecolor='black')
            plt.title('Distribuci√≥n de Direcci√≥n del Viento', fontsize=12, fontweight='bold')
            plt.xlabel('Direcci√≥n del Viento (grados)')
            plt.ylabel('Frecuencia')
            plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        print("‚úì Visualizaciones generadas exitosamente")
    def preprocess_data(self):
        """Limpieza y preparaci√≥n del dataset"""
        if self.df is None:
            print("‚ùå Primero debe cargar los datos")
            return

        # Construir un √≠ndice temporal (hora promedio del registro)
        self.df['timestamp'] = pd.to_datetime(dict(year=self.df['year'],
                                            month=self.df['month'],
                                            day=self.df['day'],
                                            hour=self.df['hour']))

        # Ordenar por el timestamp
        self.df = self.df.sort_values('timestamp').reset_index(drop=True)
        self.df.set_index('timestamp', inplace=True)

        # Aplicar interpolaci√≥n por tiempo
        self.df = self.df.interpolate(method='time')

        # Verificaci√≥n: revisar valores faltantes despu√©s de la interpolaci√≥n
        missing_after = self.df.isnull().sum()
        print("Valores faltantes despu√©s de la interpolaci√≥n:\n", missing_after)
        print("\n" + "=" * 60)

        print("METODOLOG√çA DE PREPROCESAMIENTO")
        print("=" * 60)

        # 1. Crear caracter√≠sticas temporales
        print("üîß Creando caracter√≠sticas temporales...")
        if 'hour' in self.df.columns:
            self.df['hour_sin'] = np.sin(2 * np.pi * self.df['hour'] / 24)
            self.df['hour_cos'] = np.cos(2 * np.pi * self.df['hour'] / 24)

        if 'month' in self.df.columns:
            self.df['month_sin'] = np.sin(2 * np.pi * self.df['month'] / 12)
            self.df['month_cos'] = np.cos(2 * np.pi * self.df['month'] / 12)

        # 2. Crear gradientes de temperatura
        print("üîß Calculando gradientes de temperatura...")
        temp_cols = [col for col in self.df.columns if 'temp_n' in col and col in self.df.columns]
        if len(temp_cols) >= 2:
            for i in range(len(temp_cols)-1):
                gradient_name = f'temp_gradient_{i+1}_{i+2}'
                self.df[gradient_name] = self.df[temp_cols[i+1]] - self.df[temp_cols[i]]

        # 3. Crear promedios de variables por nivel
        print("üîß Calculando promedios de variables...")
        # Promedio de temperaturas
        temp_cols_available = [col for col in temp_cols if col in self.df.columns]
        if temp_cols_available:
            self.df['temp_mean'] = self.df[temp_cols_available].mean(axis=1)
            self.df['temp_std'] = self.df[temp_cols_available].std(axis=1)

        # Promedio de vientos
        wind_cols = [col for col in self.df.columns if 'wind_n' in col and col in self.df.columns]
        if wind_cols:
            self.df['wind_mean'] = self.df[wind_cols].mean(axis=1)
            self.df['wind_std'] = self.df[wind_cols].std(axis=1)

        # Promedio de humedad
        rh_cols = [col for col in self.df.columns if 'RH_n' in col and col in self.df.columns]
        if rh_cols:
            self.df['rh_mean'] = self.df[rh_cols].mean(axis=1)
            self.df['rh_std'] = self.df[rh_cols].std(axis=1)

        # 4. Selecci√≥n de caracter√≠sticas para el modelo
        print("üîß Seleccionando caracter√≠sticas para el modelo...")

        # Definir variable objetivo
        target = 'soil_heat'
        if target not in self.df.columns:
            # Si no existe soil_heat, usar temp_n1 como ejemplo
            target = 'temp_n1'
            print(f"‚ö†Ô∏è  'soil_heat' no encontrada, usando '{target}' como variable objetivo")

        # Seleccionar caracter√≠sticas predictoras
        feature_cols = []

        # Variables temporales
        time_features = ['hour_sin', 'hour_cos', 'month_sin', 'month_cos']
        feature_cols.extend([col for col in time_features if col in self.df.columns])

        # Variables meteorol√≥gicas originales
        met_features = temp_cols + wind_cols + rh_cols
        feature_cols.extend([col for col in met_features if col in self.df.columns])

        # Variables de direcci√≥n del viento
        wind_dir_cols = [col for col in self.df.columns if 'dir_wind' in col]
        feature_cols.extend(wind_dir_cols[:2])  # Solo primeras 2

        # Variables derivadas
        derived_features = [col for col in self.df.columns if
                          ('gradient' in col or 'mean' in col or 'std' in col)]
        feature_cols.extend(derived_features)

        # Eliminar duplicados y filtrar columnas existentes
        feature_cols = list(set([col for col in feature_cols if col in self.df.columns]))

        print(f"üìã Caracter√≠sticas seleccionadas ({len(feature_cols)}):")
        for i, col in enumerate(feature_cols, 1):
            print(f"   {i:2d}. {col}")

        # 5. Preparar datos para ML
        print(f"\nüéØ Variable objetivo: {target}")

        # Crear matrices X e y
        X = self.df[feature_cols].copy()
        y = self.df[target].copy()

        # Eliminar filas con valores faltantes en y
        valid_idx = ~y.isnull()
        X = X[valid_idx]
        y = y[valid_idx]

        print(f"üìä Datos despu√©s de limpieza: {X.shape[0]} muestras, {X.shape[1]} caracter√≠sticas")

        # Imputar valores faltantes en X
        print("üîß Imputando valores faltantes...")
        imputer = SimpleImputer(strategy='median')
        X_imputed = imputer.fit_transform(X)
        X = pd.DataFrame(X_imputed, columns=feature_cols, index=X.index)

        # Divisi√≥n train/test
        print("üîß Dividiendo datos en entrenamiento y prueba (80/20)...")
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, shuffle=True
        )

        # Normalizaci√≥n
        print("üîß Normalizando caracter√≠sticas...")
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        # Convertir de vuelta a DataFrame
        self.X_train_scaled = pd.DataFrame(self.X_train_scaled, columns=feature_cols, index=self.X_train.index)
        self.X_test_scaled = pd.DataFrame(self.X_test_scaled, columns=feature_cols, index=self.X_test.index)

        print(f"‚úì Preprocesamiento completado:")
        print(f"   ‚Ä¢ Entrenamiento: {self.X_train.shape[0]} muestras")
        print(f"   ‚Ä¢ Prueba: {self.X_test.shape[0]} muestras")
        print(f"   ‚Ä¢ Caracter√≠sticas: {self.X_train.shape[1]}")

        return self.X_train_scaled, self.X_test_scaled, self.y_train, self.y_test

    def train_models(self):
        """Entrenamiento de modelos de aprendizaje autom√°tico"""
        if self.X_train is None:
            print("‚ùå Primero debe preprocesar los datos")
            return

        print("\n" + "=" * 60)
        print("ENTRENAMIENTO DE MODELOS")
        print("=" * 60)

        # Definir modelos
        models_config = {
            'Linear Regression': LinearRegression(),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
        }

        print(f"ü§ñ Entrenando {len(models_config)} modelos...")

        for name, model in models_config.items():
            print(f"\nüîÑ Entrenando {name}...")

            # Entrenar modelo
            if name == 'Linear Regression':
                model.fit(self.X_train_scaled, self.y_train)
                train_pred = model.predict(self.X_train_scaled)
                test_pred = model.predict(self.X_test_scaled)
            else:
                model.fit(self.X_train, self.y_train)
                train_pred = model.predict(self.X_train)
                test_pred = model.predict(self.X_test)

            # Calcular m√©tricas
            train_mse = mean_squared_error(self.y_train, train_pred)
            test_mse = mean_squared_error(self.y_test, test_pred)
            train_r2 = r2_score(self.y_train, train_pred)
            test_r2 = r2_score(self.y_test, test_pred)
            train_mae = mean_absolute_error(self.y_train, train_pred)
            test_mae = mean_absolute_error(self.y_test, test_pred)

            # Guardar modelo y resultados
            self.models[name] = model
            self.results[name] = {
                'train_mse': train_mse,
                'test_mse': test_mse,
                'train_r2': train_r2,
                'test_r2': test_r2,
                'train_mae': train_mae,
                'test_mae': test_mae,
                'train_pred': train_pred,
                'test_pred': test_pred
            }

            print(f"   ‚úì R¬≤: {test_r2:.4f} | MSE: {test_mse:.4f} | MAE: {test_mae:.4f}")

        print(f"\n‚úÖ Entrenamiento completado para todos los modelos")

    def show_results(self):
        """Presentaci√≥n de resultados mediante m√©tricas"""
        if not self.results:
            print("‚ùå Primero debe entrenar los modelos")
            return

        print("\n" + "=" * 60)
        print("RESULTADOS PRELIMINARES")
        print("=" * 60)

        # Tabla de resultados
        results_df = pd.DataFrame({
            'Modelo': list(self.results.keys()),
            'R¬≤ Train': [self.results[model]['train_r2'] for model in self.results.keys()],
            'R¬≤ Test': [self.results[model]['test_r2'] for model in self.results.keys()],
            'MSE Train': [self.results[model]['train_mse'] for model in self.results.keys()],
            'MSE Test': [self.results[model]['test_mse'] for model in self.results.keys()],
            'MAE Train': [self.results[model]['train_mae'] for model in self.results.keys()],
            'MAE Test': [self.results[model]['test_mae'] for model in self.results.keys()]
        })

        print("üìä M√âTRICAS DE RENDIMIENTO:")
        print(results_df.round(4).to_string(index=False))

        # Identificar mejor modelo
        best_model = results_df.loc[results_df['R¬≤ Test'].idxmax(), 'Modelo']
        best_r2 = results_df.loc[results_df['R¬≤ Test'].idxmax(), 'R¬≤ Test']

        print(f"\nüèÜ MEJOR MODELO: {best_model}")
        print(f"   ‚Ä¢ R¬≤ en prueba: {best_r2:.4f}")
        print(f"   ‚Ä¢ MSE en prueba: {self.results[best_model]['test_mse']:.4f}")
        print(f"   ‚Ä¢ MAE en prueba: {self.results[best_model]['test_mae']:.4f}")

        # Visualizaciones de resultados
        self.plot_results()

        # Importancia de caracter√≠sticas (si disponible)
        if best_model in ['Random Forest', 'Gradient Boosting']:
            self.plot_feature_importance(best_model)

        return results_df

    def plot_results(self):
        """Crear visualizaciones de resultados"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. Comparaci√≥n de R¬≤
        models = list(self.results.keys())
        train_r2 = [self.results[model]['train_r2'] for model in models]
        test_r2 = [self.results[model]['test_r2'] for model in models]

        x = np.arange(len(models))
        width = 0.35

        axes[0, 0].bar(x - width/2, train_r2, width, label='Entrenamiento', alpha=0.8)
        axes[0, 0].bar(x + width/2, test_r2, width, label='Prueba', alpha=0.8)
        axes[0, 0].set_xlabel('Modelos')
        axes[0, 0].set_ylabel('R¬≤ Score')
        axes[0, 0].set_title('Comparaci√≥n de R¬≤ por Modelo')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(models, rotation=45)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. Predicciones vs Valores Reales (mejor modelo)
        best_model = max(self.results.keys(), key=lambda k: self.results[k]['test_r2'])
        test_pred = self.results[best_model]['test_pred']

        axes[0, 1].scatter(self.y_test, test_pred, alpha=0.6, s=20)
        axes[0, 1].plot([self.y_test.min(), self.y_test.max()],
                       [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        axes[0, 1].set_xlabel('Valores Reales')
        axes[0, 1].set_ylabel('Predicciones')
        axes[0, 1].set_title(f'Predicciones vs Reales - {best_model}')
        axes[0, 1].grid(True, alpha=0.3)

        # 3. Residuos
        residuals = self.y_test - test_pred
        axes[1, 0].scatter(test_pred, residuals, alpha=0.6, s=20)
        axes[1, 0].axhline(y=0, color='r', linestyle='--')
        axes[1, 0].set_xlabel('Predicciones')
        axes[1, 0].set_ylabel('Residuos')
        axes[1, 0].set_title(f'An√°lisis de Residuos - {best_model}')
        axes[1, 0].grid(True, alpha=0.3)

        # 4. Distribuci√≥n de errores
        axes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')
        axes[1, 1].set_xlabel('Residuos')
        axes[1, 1].set_ylabel('Frecuencia')
        axes[1, 1].set_title('Distribuci√≥n de Residuos')
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def plot_feature_importance(self, model_name):
        """Graficar importancia de caracter√≠sticas"""
        if model_name not in self.models:
            return

        model = self.models[model_name]
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            feature_names = self.X_train.columns

            # Ordenar por importancia
            indices = np.argsort(importances)[::-1]

            plt.figure(figsize=(12, 8))
            plt.title(f'Importancia de Caracter√≠sticas - {model_name}')
            plt.bar(range(min(20, len(importances))),
                   importances[indices[:20]])
            plt.xticks(range(min(20, len(importances))),
                      [feature_names[i] for i in indices[:20]],
                      rotation=45, ha='right')
            plt.xlabel('Caracter√≠sticas')
            plt.ylabel('Importancia')
            plt.tight_layout()
            plt.show()

    def run_complete_analysis(self):
        """Ejecutar an√°lisis completo"""
        print("üöÄ INICIANDO AN√ÅLISIS COMPLETO DE MACHINE LEARNING")
        print("üéØ PROBLEMA: Regresi√≥n para predicci√≥n de flujo de calor del suelo")
        print("üìç UBICACI√ìN: Torre de Gradiente LAMAR, Jun√≠n, Per√∫")

        # Ejecutar todos los pasos
        self.load_and_explore_data()
        self.create_visualizations()
        self.preprocess_data()
        self.train_models()
        results = self.show_results()

        print("\n" + "=" * 60)
        print("RESUMEN EJECUTIVO")
        print("=" * 60)
        print("‚úÖ An√°lisis exploratorio completado con visualizaciones")
        print("‚úÖ Preprocesamiento realizado con ingenier√≠a de caracter√≠sticas")
        print("‚úÖ M√∫ltiples modelos entrenados y evaluados")
        print("‚úÖ Resultados preliminares presentados con m√©tricas apropiadas")

        return results

# INSTRUCCIONES DE USO:
# ===================
# 1. Guarda este c√≥digo en un archivo .py
# 2. Aseg√∫rate de tener tu archivo CSV de datos
# 3. Ejecuta el siguiente c√≥digo:

if __name__ == "__main__":
    # Cambia la ruta por la ubicaci√≥n de tu archivo CSV
    CSV_PATH = "IGP.csv"  # ‚Üê CAMBIAR ESTA RUTA

    # Crear instancia y ejecutar an√°lisis
    analyzer = TorreGradienteML(CSV_PATH)
    results = analyzer.run_complete_analysis()

    print(f"\nüéâ An√°lisis completado exitosamente!")
    print(f"üìä Revisa las visualizaciones y m√©tricas generadas")

